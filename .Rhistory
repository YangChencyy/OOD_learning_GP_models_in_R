# distance = KL(u1, u2, std1, std2)
# return(distance)
}
model_fit_test <- function(trainset = "MNIST", testsets = c("FashionMNIST"), n_tr = 1000, n_ts = 1000, f = 16){
df = read.csv(paste0("data_", toString(f), "/", trainset, "/train.csv"))[,-1]
set.seed(100)
# Train dataset
select.index <- sample(1:nrow(df), n_tr, replace = FALSE)
train.df <- df[select.index, ]
CNN_train_score = train.df[, (f+1):(f+10)]
CNN_train_score = normalize.rows(as.matrix(exp(CNN_train_score)), method = "manhattan") #normalize
Y <- rep(0,nrow(train.df))
for (i in 1:10){  # weighted average of Y
Y <- Y + train.df[, (f+i)] * CNN_train_score[,i]
}
models = vector("list", 10)
cv_results = vector("list", 10)
# 10 different clusters
foreach (i=1:10) %do% {
#for(i in 1:10){
X <- train.df[train.df[,"label"]==i-1, 1:f]
y <- train.df[train.df[,"label"]==i-1, (f+i)] # only look at the scores of correct label
fit <- mlegp(data.matrix(X), data.matrix(y))
cv_results[[i]] = CV(fit)  # mean # VARIANCE
cv_results[[i]][,2] = sqrt(cv_results[[i]][,2]) # calculate std based on variance
models[[i]] = fit
rm(fit)
}
print("Train Finished")
directory_path <- paste0("Rdata_", toString(n_tr), "_", toString(f))
# Use dir.create() to create the directory
if (!dir.exists(directory_path)) {
if (dir.create(directory_path)) {
cat("Directory created successfully:", directory_path, "\n")
} else {
cat("Failed to create the directory:", directory_path, "\n")
}
} else {
cat("Directory already exists:", directory_path, "\n")
}
for (testset in testsets){
results_test = vector("list", 10)
results_ood = vector("list", 10)
t.df = read.csv(paste0("data_", toString(f), "/", trainset, "/", testset, "_test.csv"))[,-1]
test.df = t.df[t.df[,"class"]=='test', ]
ood.df = t.df[t.df[,"class"]=='OOD', ]
test.X <- test.df[1:n_ts, 1:f]
OOD.X <- ood.df[1:n_ts, 1:f]
CNN_test_score = test.df[1:n_ts, (f+1):(f+10)]
CNN_test_score = normalize.rows(as.matrix(exp(CNN_test_score)), method = "manhattan") #normalize
CNN_OOD_score = ood.df[1:n_ts, (f+1):(f+10)]
CNN_OOD_score = normalize.rows(as.matrix(exp(CNN_OOD_score)), method = "manhattan") #normalize
for(i in 1:10){  # 10 different clusters
fit = models[[i]]
results_test[[i]] <- predict(fit, data.matrix(test.X),se.fit=TRUE)
results_ood[[i]] <- predict(fit, data.matrix(OOD.X),se.fit=TRUE)
}
# compute the predictive mean
y.test <- y.ood <- s2.test <- s2.ood <- rep(0, n_ts)
for(i in 1:10){
y.test <- y.test + results_test[[i]]$fit * CNN_test_score[,i]
y.ood <- y.ood + results_ood[[i]]$fit * CNN_OOD_score[,i]
}
# compute the predictive variance
for(i in 1:10){
s2.test <- s2.test + (results_test[[i]]$se.fit^2 + results_test[[i]]$fit^2) * CNN_test_score[,i]
s2.ood <- s2.ood + (results_ood[[i]]$se.fit^2 + results_ood[[i]]$fit^2) * CNN_OOD_score[,i]
}
test.df = test.df[1:n_ts, ]
ood.df = ood.df[1:n_ts, ]
save(results_test, test.df, results_ood, ood.df, cv_results,
file=paste0("Rdata_", toString(n_tr), "_", toString(f), "/", trainset, "_", testset, ".RData"))
}
print("Test Finished")
# save(Y, models, file=paste0("Rdata/", trainset, "_models.RData"))
}
get_argmax <- function(x) {
return(which.max(x) - 1)
}
score_function <- function(trainset = "MNIST", testset = "FashionMNIST", q = 0.9, f = 16, n_tr = 1000, n_ts = 1000){
load(file=paste0("Rdata_", toString(n_tr), "_", toString(f), "/", trainset, "_", testset, ".RData"))
test.df$predictions = apply(test.df[, (f+1):(f+10)], 1, get_argmax) + 1
ood.df$predictions = apply(ood.df[, (f+1):(f+10)], 1, get_argmax) + 1
test.df$KL = 0
ood.df$KL = 0
test.df$mean = 0
ood.df$mean = 0
test.df$std = 0
ood.df$std = 0
for (i in (1:nrow(test.df))){
class = test.df$predictions[i]
test.df$mean[i] = results_test[[class]]$fit[i]
test.df$std[i] = results_test[[class]]$se.fit[i]
test.df$KL[i] = KL_all(cv_results[[class]][,1], results_test[[class]]$fit[i], # mean
cv_results[[class]][,2], results_test[[class]]$se.fit[i]) # SD
}
# Calculate KL for train data
KL_list = c()
for (i in (1:10)){
cv_train = cv_results[[i]]
kl_ = c()
for (j in (1:nrow(cv_train))){
kl_train = KL_all(cv_train[,1], cv_train[j,1], cv_train[,2], cv_train[j,2])
kl_ = c(kl_, kl_train)
}
KL_list = c(KL_list, quantile(kl_, q))
}
for (i in (1:nrow(ood.df))){
class = ood.df$predictions[i]
ood.df$mean[i] = results_ood[[class]]$fit[i]
ood.df$std[i] = results_ood[[class]]$se.fit[i]
ood.df$KL[i] = KL_all(cv_results[[class]][,1], results_ood[[class]]$fit[i], # mean
cv_results[[class]][,2], results_ood[[class]]$se.fit[i]) # SD
}
ID_acc_list = c()
OOD_acc_list = c()
ID_sum = 0
OOD_sum = 0
for (i in 1:10){
# cv_std = quantile(cv_results[[i]][,2], 0.9)
# ID_acc = mean(test.df[test.df$predictions == i, ]$std < cv_std)
# ID_sum = ID_sum + sum(test.df[test.df$predictions == i, ]$std < cv_std)
ID_acc = mean(test.df[test.df$predictions == i, ]$KL < KL_list[i])
ID_sum = ID_sum + sum(test.df[test.df$predictions == i, ]$KL < KL_list[i])
ID_acc_list = c(ID_acc_list, ID_acc)
OOD_acc = mean(ood.df[ood.df$predictions == i, ]$KL > KL_list[i])
OOD_sum = OOD_sum + sum(ood.df[ood.df$predictions == i, ]$KL > KL_list[i])
OOD_acc_list = c(OOD_acc_list, OOD_acc)
}
# Create a list to store the dataframes
result <- list(test.df = test.df, ood.df = ood.df, cv_results = cv_results,
ID_acc = ID_acc_list, OOD_acc = OOD_acc_list, ID_all = ID_sum/n_ts, OOD_all = OOD_sum/n_ts)
return(result)
}
# For plot
colors <- c("rgba(31, 119, 180, 0.6)", "rgba(255, 127, 14, 0.6)", "rgba(44, 160, 44, 0.6)")
hist_KL <- function(distance, X2, i, r = 500){
fig <- plot_ly(alpha = 0.6)
fig <- fig %>% add_histogram(x = ~distance, name = "Test - ID", marker = list(color = colors[1]))
fig <- fig %>% add_histogram(x = ~X2, name = "Test - OOD", marker = list(color = colors[2]))
fig <- fig %>% layout(barmode = "overlay",
title = paste0('Model', i),
xaxis = list(
range = c(0, r)
)
)
return(fig)
}
hist_meanSD <- function(X, X2, X3, i){
fig <- plot_ly(alpha = 0.6)
fig <- fig %>% add_histogram(x = ~X, name = "Test - ID", marker = list(color = colors[1]))
fig <- fig %>% add_histogram(x = ~X2, name = "Test - OOD", marker = list(color = colors[2]))
fig <- fig %>% add_histogram(x = ~X3, name = "Train - CV", marker = list(color = colors[3]))
fig <- fig %>% layout(barmode = "overlay",
title = paste0('Model', i)
)
return(fig)
}
hist_plot_meanSD <- function(test.df, ood.df, cv_results, name, feature = "mean", r = 30){
if (feature == "mean"){
col = 1
}else{
col = 2
}
#pl = vector("list", 9)
# for (i in 1:9){
#   p = hist_meanSD(test.df[[feature]][test.df$predictions == i],
#             ood.df[[feature]][ood.df$predictions == i],
#             cv_results[[i]][,col],
#             i, r)
#   pl[[i]] = p
# }
p1 = hist_meanSD(test.df[[feature]][test.df$predictions == 1], ood.df[[feature]][ood.df$predictions == 1], cv_results[[1]][,col], 1)
p2 = hist_meanSD(test.df[[feature]][test.df$predictions == 2], ood.df[[feature]][ood.df$predictions == 2], cv_results[[2]][,col], 2)
p3 = hist_meanSD(test.df[[feature]][test.df$predictions == 3], ood.df[[feature]][ood.df$predictions == 3], cv_results[[3]][,col], 3)
p4 = hist_meanSD(test.df[[feature]][test.df$predictions == 4], ood.df[[feature]][ood.df$predictions == 4], cv_results[[4]][,col], 4)
p5 = hist_meanSD(test.df[[feature]][test.df$predictions == 5], ood.df[[feature]][ood.df$predictions == 5], cv_results[[5]][,col], 5)
p6 = hist_meanSD(test.df[[feature]][test.df$predictions == 6], ood.df[[feature]][ood.df$predictions == 6], cv_results[[6]][,col], 6)
p7 = hist_meanSD(test.df[[feature]][test.df$predictions == 7], ood.df[[feature]][ood.df$predictions == 7], cv_results[[7]][,col], 7)
p8 = hist_meanSD(test.df[[feature]][test.df$predictions == 8], ood.df[[feature]][ood.df$predictions == 8], cv_results[[8]][,col], 8)
p9 = hist_meanSD(test.df[[feature]][test.df$predictions == 9], ood.df[[feature]][ood.df$predictions == 9], cv_results[[9]][,col], 9)
# Combine the plots using subplot function
combined_figure <- subplot(p1, p2, p3, p4, p5, p6, p7, p8, p9, nrows = 3, shareX = TRUE, shareY = TRUE)
combined_figure <- layout(
combined_figure,
title = paste0(feature, " Plot -", name),
titlefont = list(size = 16)  # You can adjust the font size as needed
)
# Show the combined figure
return(combined_figure)
}
hist_plot_KL <- function(test.df, ood.df, name, feature = "KL", r = 500){
# pl = vector("list", 9)
# for (i in 1:9){
#   p = hist_KL(test.df[[feature]][test.df$predictions == i], ood.df[[feature]][ood.df$predictions == i], i, r)
#   pl[[i]] = p
#   rm(p)
# }
p1 = hist_KL(test.df[[feature]][test.df$predictions == 1], ood.df[[feature]][ood.df$predictions == 1], 1, r)
p2 = hist_KL(test.df[[feature]][test.df$predictions == 2], ood.df[[feature]][ood.df$predictions == 2], 2, r)
p3 = hist_KL(test.df[[feature]][test.df$predictions == 3], ood.df[[feature]][ood.df$predictions == 3], 3, r)
p4 = hist_KL(test.df[[feature]][test.df$predictions == 4], ood.df[[feature]][ood.df$predictions == 4], 4, r)
p5 = hist_KL(test.df[[feature]][test.df$predictions == 5], ood.df[[feature]][ood.df$predictions == 5], 5, r)
p6 = hist_KL(test.df[[feature]][test.df$predictions == 6], ood.df[[feature]][ood.df$predictions == 6], 6, r)
p7 = hist_KL(test.df[[feature]][test.df$predictions == 7], ood.df[[feature]][ood.df$predictions == 7], 7, r)
p8 = hist_KL(test.df[[feature]][test.df$predictions == 8], ood.df[[feature]][ood.df$predictions == 8], 8, r)
p9 = hist_KL(test.df[[feature]][test.df$predictions == 9], ood.df[[feature]][ood.df$predictions == 9], 9, r)
# Combine the plots using subplot function
combined_figure <- subplot(p1, p2, p3, p4, p5, p6, p7, p8, p9, nrows = 3, shareX = TRUE, shareY = TRUE)
combined_figure <- layout(
combined_figure,
title = paste0(feature, " Plot - ", name),
titlefont = list(size = 16)  # You can adjust the font size as needed
)
# Show the combined figure
return(combined_figure)
}
KL1 <- function(u1, u2, std1, std2){
kl_divergence <- log(std2 / std1)
return(kl_divergence)
}
KL2 <- function(u1, u2, std1, std2){
kl_divergence <- (std1^2 + (u1 - u2)^2) / (2 * std2^2) - 0.5
return(kl_divergence)
}
KL_all1 <- function(u1_list, u2, std1_list, std2){
KLs = c()
for (i in 1:length(u1_list)){
distance = KL1(u1_list[i], u2, std1_list[i], std2)
KLs = c(KLs, distance)
}
return(mean(KLs))
}
KL_all2 <- function(u1_list, u2, std1_list, std2){
KLs = c()
for (i in 1:length(u1_list)){
distance = KL2(u1_list[i], u2, std1_list[i], std2)
KLs = c(KLs, distance)
}
return(mean(KLs))
}
InD_Dataset = "MNIST"
OOD_Datasets = c("FashionMNIST", "Cifar_10", "SVHN", "Imagenet_r", "Imagenet_c")
model_fit_test(trainset = InD_Dataset, testsets = OOD_Datasets, n_tr = 1000, n_ts = 1000, f = 16)  # Run only once, specify the training samples to use
library(reticulate)
library(plotly)
library(ramify)
library(mlegp)
library(wordspace)
library(rmarkdown)
library(MASS)
library(foreach)
library(umap)
KL <- function(u1, u2, std1, std2){
kl_divergence <- log(std1 / std2) + (std2^2 + (u1 - u2)^2) / (2 * std1^2) - 0.5
return(kl_divergence)
}
KL_all <- function(u1_list, u2, std1_list, std2){
# if first calculate KL distance then take the mean
KLs = c()
for (i in 1:length(u1_list)){
distance = KL(u1_list[i], u2, std1_list[i], std2)
KLs = c(KLs, distance)
}
return(mean(KLs, na.rm = TRUE))
# # if first calculate mean of cv_mean and cv_std, then calculate KL distance once
# u1 = mean(u1_list)
# std1 = mean(std1_list)
# distance = KL(u1, u2, std1, std2)
# return(distance)
}
model_fit_test <- function(trainset = "MNIST", testsets = c("FashionMNIST"), n_tr = 1000, n_ts = 1000, f = 16){
df = read.csv(paste0("data_", toString(f), "/", trainset, "/train.csv"))[,-1]
set.seed(100)
# Train dataset
select.index <- sample(1:nrow(df), n_tr, replace = FALSE)
train.df <- df[select.index, ]
CNN_train_score = train.df[, (f+1):(f+10)]
CNN_train_score = normalize.rows(as.matrix(exp(CNN_train_score)), method = "manhattan") #normalize
Y <- rep(0,nrow(train.df))
for (i in 1:10){  # weighted average of Y
Y <- Y + train.df[, (f+i)] * CNN_train_score[,i]
}
models = vector("list", 10)
cv_results = vector("list", 10)
# 10 different clusters
foreach (i=1:10) %do% {
#for(i in 1:10){
X <- train.df[train.df[,"label"]==i-1, 1:f]
y <- train.df[train.df[,"label"]==i-1, (f+i)] # only look at the scores of correct label
fit <- mlegp(data.matrix(X), data.matrix(y))
cv_results[[i]] = CV(fit)  # mean # VARIANCE
cv_results[[i]][,2] = sqrt(cv_results[[i]][,2]) # calculate std based on variance
models[[i]] = fit
rm(fit)
}
print("Train Finished")
directory_path <- paste0("Rdata_", toString(n_tr), "_", toString(f))
# Use dir.create() to create the directory
if (!dir.exists(directory_path)) {
if (dir.create(directory_path)) {
cat("Directory created successfully:", directory_path, "\n")
} else {
cat("Failed to create the directory:", directory_path, "\n")
}
} else {
cat("Directory already exists:", directory_path, "\n")
}
for (testset in testsets){
results_test = vector("list", 10)
results_ood = vector("list", 10)
t.df = read.csv(paste0("data_", toString(f), "/", trainset, "/", testset, "_test.csv"))[,-1]
test.df = t.df[t.df[,"class"]=='test', ]
ood.df = t.df[t.df[,"class"]=='OOD', ]
test.X <- test.df[1:n_ts, 1:f]
OOD.X <- ood.df[1:n_ts, 1:f]
CNN_test_score = test.df[1:n_ts, (f+1):(f+10)]
CNN_test_score = normalize.rows(as.matrix(exp(CNN_test_score)), method = "manhattan") #normalize
CNN_OOD_score = ood.df[1:n_ts, (f+1):(f+10)]
CNN_OOD_score = normalize.rows(as.matrix(exp(CNN_OOD_score)), method = "manhattan") #normalize
for(i in 1:10){  # 10 different clusters
fit = models[[i]]
results_test[[i]] <- predict(fit, data.matrix(test.X),se.fit=TRUE)
results_ood[[i]] <- predict(fit, data.matrix(OOD.X),se.fit=TRUE)
}
# compute the predictive mean
y.test <- y.ood <- s2.test <- s2.ood <- rep(0, n_ts)
for(i in 1:10){
y.test <- y.test + results_test[[i]]$fit * CNN_test_score[,i]
y.ood <- y.ood + results_ood[[i]]$fit * CNN_OOD_score[,i]
}
# compute the predictive variance
for(i in 1:10){
s2.test <- s2.test + (results_test[[i]]$se.fit^2 + results_test[[i]]$fit^2) * CNN_test_score[,i]
s2.ood <- s2.ood + (results_ood[[i]]$se.fit^2 + results_ood[[i]]$fit^2) * CNN_OOD_score[,i]
}
test.df = test.df[1:n_ts, ]
ood.df = ood.df[1:n_ts, ]
save(results_test, test.df, results_ood, ood.df, cv_results,
file=paste0("Rdata_", toString(n_tr), "_", toString(f), "/", trainset, "_", testset, ".RData"))
}
print("Test Finished")
# save(Y, models, file=paste0("Rdata/", trainset, "_models.RData"))
}
get_argmax <- function(x) {
return(which.max(x) - 1)
}
score_function <- function(trainset = "MNIST", testset = "FashionMNIST", q = 0.9, f = 16, n_tr = 1000, n_ts = 1000){
load(file=paste0("Rdata_", toString(n_tr), "_", toString(f), "/", trainset, "_", testset, ".RData"))
test.df$predictions = apply(test.df[, (f+1):(f+10)], 1, get_argmax) + 1
ood.df$predictions = apply(ood.df[, (f+1):(f+10)], 1, get_argmax) + 1
test.df$KL = 0
ood.df$KL = 0
test.df$mean = 0
ood.df$mean = 0
test.df$std = 0
ood.df$std = 0
for (i in (1:nrow(test.df))){
class = test.df$predictions[i]
test.df$mean[i] = results_test[[class]]$fit[i]
test.df$std[i] = results_test[[class]]$se.fit[i]
test.df$KL[i] = KL_all(cv_results[[class]][,1], results_test[[class]]$fit[i], # mean
cv_results[[class]][,2], results_test[[class]]$se.fit[i]) # SD
}
# Calculate KL for train data
KL_list = c()
for (i in (1:10)){
cv_train = cv_results[[i]]
kl_ = c()
for (j in (1:nrow(cv_train))){
kl_train = KL_all(cv_train[,1], cv_train[j,1], cv_train[,2], cv_train[j,2])
kl_ = c(kl_, kl_train)
}
KL_list = c(KL_list, quantile(kl_, q))
}
for (i in (1:nrow(ood.df))){
class = ood.df$predictions[i]
ood.df$mean[i] = results_ood[[class]]$fit[i]
ood.df$std[i] = results_ood[[class]]$se.fit[i]
ood.df$KL[i] = KL_all(cv_results[[class]][,1], results_ood[[class]]$fit[i], # mean
cv_results[[class]][,2], results_ood[[class]]$se.fit[i]) # SD
}
ID_acc_list = c()
OOD_acc_list = c()
ID_sum = 0
OOD_sum = 0
for (i in 1:10){
# cv_std = quantile(cv_results[[i]][,2], 0.9)
# ID_acc = mean(test.df[test.df$predictions == i, ]$std < cv_std)
# ID_sum = ID_sum + sum(test.df[test.df$predictions == i, ]$std < cv_std)
ID_acc = mean(test.df[test.df$predictions == i, ]$KL < KL_list[i])
ID_sum = ID_sum + sum(test.df[test.df$predictions == i, ]$KL < KL_list[i])
ID_acc_list = c(ID_acc_list, ID_acc)
OOD_acc = mean(ood.df[ood.df$predictions == i, ]$KL > KL_list[i])
OOD_sum = OOD_sum + sum(ood.df[ood.df$predictions == i, ]$KL > KL_list[i])
OOD_acc_list = c(OOD_acc_list, OOD_acc)
}
# Create a list to store the dataframes
result <- list(test.df = test.df, ood.df = ood.df, cv_results = cv_results,
ID_acc = ID_acc_list, OOD_acc = OOD_acc_list, ID_all = ID_sum/n_ts, OOD_all = OOD_sum/n_ts)
return(result)
}
# For plot
colors <- c("rgba(31, 119, 180, 0.6)", "rgba(255, 127, 14, 0.6)", "rgba(44, 160, 44, 0.6)")
hist_KL <- function(distance, X2, i, r = 500){
fig <- plot_ly(alpha = 0.6)
fig <- fig %>% add_histogram(x = ~distance, name = "Test - ID", marker = list(color = colors[1]))
fig <- fig %>% add_histogram(x = ~X2, name = "Test - OOD", marker = list(color = colors[2]))
fig <- fig %>% layout(barmode = "overlay",
title = paste0('Model', i),
xaxis = list(
range = c(0, r)
)
)
return(fig)
}
hist_meanSD <- function(X, X2, X3, i){
fig <- plot_ly(alpha = 0.6)
fig <- fig %>% add_histogram(x = ~X, name = "Test - ID", marker = list(color = colors[1]))
fig <- fig %>% add_histogram(x = ~X2, name = "Test - OOD", marker = list(color = colors[2]))
fig <- fig %>% add_histogram(x = ~X3, name = "Train - CV", marker = list(color = colors[3]))
fig <- fig %>% layout(barmode = "overlay",
title = paste0('Model', i)
)
return(fig)
}
hist_plot_meanSD <- function(test.df, ood.df, cv_results, name, feature = "mean", r = 30){
if (feature == "mean"){
col = 1
}else{
col = 2
}
#pl = vector("list", 9)
# for (i in 1:9){
#   p = hist_meanSD(test.df[[feature]][test.df$predictions == i],
#             ood.df[[feature]][ood.df$predictions == i],
#             cv_results[[i]][,col],
#             i, r)
#   pl[[i]] = p
# }
p1 = hist_meanSD(test.df[[feature]][test.df$predictions == 1], ood.df[[feature]][ood.df$predictions == 1], cv_results[[1]][,col], 1)
p2 = hist_meanSD(test.df[[feature]][test.df$predictions == 2], ood.df[[feature]][ood.df$predictions == 2], cv_results[[2]][,col], 2)
p3 = hist_meanSD(test.df[[feature]][test.df$predictions == 3], ood.df[[feature]][ood.df$predictions == 3], cv_results[[3]][,col], 3)
p4 = hist_meanSD(test.df[[feature]][test.df$predictions == 4], ood.df[[feature]][ood.df$predictions == 4], cv_results[[4]][,col], 4)
p5 = hist_meanSD(test.df[[feature]][test.df$predictions == 5], ood.df[[feature]][ood.df$predictions == 5], cv_results[[5]][,col], 5)
p6 = hist_meanSD(test.df[[feature]][test.df$predictions == 6], ood.df[[feature]][ood.df$predictions == 6], cv_results[[6]][,col], 6)
p7 = hist_meanSD(test.df[[feature]][test.df$predictions == 7], ood.df[[feature]][ood.df$predictions == 7], cv_results[[7]][,col], 7)
p8 = hist_meanSD(test.df[[feature]][test.df$predictions == 8], ood.df[[feature]][ood.df$predictions == 8], cv_results[[8]][,col], 8)
p9 = hist_meanSD(test.df[[feature]][test.df$predictions == 9], ood.df[[feature]][ood.df$predictions == 9], cv_results[[9]][,col], 9)
# Combine the plots using subplot function
combined_figure <- subplot(p1, p2, p3, p4, p5, p6, p7, p8, p9, nrows = 3, shareX = TRUE, shareY = TRUE)
combined_figure <- layout(
combined_figure,
title = paste0(feature, " Plot -", name),
titlefont = list(size = 16)  # You can adjust the font size as needed
)
# Show the combined figure
return(combined_figure)
}
hist_plot_KL <- function(test.df, ood.df, name, feature = "KL", r = 500){
# pl = vector("list", 9)
# for (i in 1:9){
#   p = hist_KL(test.df[[feature]][test.df$predictions == i], ood.df[[feature]][ood.df$predictions == i], i, r)
#   pl[[i]] = p
#   rm(p)
# }
p1 = hist_KL(test.df[[feature]][test.df$predictions == 1], ood.df[[feature]][ood.df$predictions == 1], 1, r)
p2 = hist_KL(test.df[[feature]][test.df$predictions == 2], ood.df[[feature]][ood.df$predictions == 2], 2, r)
p3 = hist_KL(test.df[[feature]][test.df$predictions == 3], ood.df[[feature]][ood.df$predictions == 3], 3, r)
p4 = hist_KL(test.df[[feature]][test.df$predictions == 4], ood.df[[feature]][ood.df$predictions == 4], 4, r)
p5 = hist_KL(test.df[[feature]][test.df$predictions == 5], ood.df[[feature]][ood.df$predictions == 5], 5, r)
p6 = hist_KL(test.df[[feature]][test.df$predictions == 6], ood.df[[feature]][ood.df$predictions == 6], 6, r)
p7 = hist_KL(test.df[[feature]][test.df$predictions == 7], ood.df[[feature]][ood.df$predictions == 7], 7, r)
p8 = hist_KL(test.df[[feature]][test.df$predictions == 8], ood.df[[feature]][ood.df$predictions == 8], 8, r)
p9 = hist_KL(test.df[[feature]][test.df$predictions == 9], ood.df[[feature]][ood.df$predictions == 9], 9, r)
# Combine the plots using subplot function
combined_figure <- subplot(p1, p2, p3, p4, p5, p6, p7, p8, p9, nrows = 3, shareX = TRUE, shareY = TRUE)
combined_figure <- layout(
combined_figure,
title = paste0(feature, " Plot - ", name),
titlefont = list(size = 16)  # You can adjust the font size as needed
)
# Show the combined figure
return(combined_figure)
}
KL1 <- function(u1, u2, std1, std2){
kl_divergence <- log(std2 / std1)
return(kl_divergence)
}
KL2 <- function(u1, u2, std1, std2){
kl_divergence <- (std1^2 + (u1 - u2)^2) / (2 * std2^2) - 0.5
return(kl_divergence)
}
KL_all1 <- function(u1_list, u2, std1_list, std2){
KLs = c()
for (i in 1:length(u1_list)){
distance = KL1(u1_list[i], u2, std1_list[i], std2)
KLs = c(KLs, distance)
}
return(mean(KLs))
}
KL_all2 <- function(u1_list, u2, std1_list, std2){
KLs = c()
for (i in 1:length(u1_list)){
distance = KL2(u1_list[i], u2, std1_list[i], std2)
KLs = c(KLs, distance)
}
return(mean(KLs))
}
InD_Dataset = "MNIST"
OOD_Datasets = c("FashionMNIST", "Cifar_10", "SVHN", "Imagenet_r", "Imagenet_c")
model_fit_test(trainset = InD_Dataset, testsets = OOD_Datasets, n_tr = 1000, n_ts = 1000, f = 16)  # Run only once, specify the training samples to use
